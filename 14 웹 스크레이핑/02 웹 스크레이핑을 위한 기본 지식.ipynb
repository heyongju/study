{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터의 요청과 응답 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML의 기본 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing C:\\myPycode\\HTML_example.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile C:\\myPycode\\HTML_example.html\n",
    "<!doctype html>\n",
    "<html>\n",
    "    <head>\n",
    "        <meta charset='utf-8'>\n",
    "        <title>이것은 HTML 예제</title>\n",
    "    </head>\n",
    "    <body>\n",
    "    <h1>출간된 책 정보</h1>\n",
    "    <p id='book_title'>이해가 쏙쏙 되는 파이썬</p>\n",
    "    <p id='author'>홍길동</p>\n",
    "    <p id='publisher'>위키북스 출판사</p>\n",
    "    <p id='year'>2018</p>\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing C:\\myPycode\\HTML_example2.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile C:\\myPycode\\HTML_example2.html\n",
    "<!doctype html>\n",
    "<html>\n",
    "    <head>\n",
    "        <meta charset='utf-8'>\n",
    "        <title>이것은 HTML 예제</title>\n",
    "    </head>\n",
    "    <body>\n",
    "    <h1>출간된 책 정보</h1>\n",
    "    <p>이해가 쏙쏙 되는 파이썬</p>\n",
    "    <p>홍길동</p>\n",
    "    <p>위키북스 출판사</p>\n",
    "    <p>2018</p>\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 웹 페이지의 HTML 소스 갖고 오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get('http://www.google.co.kr')\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!doctype html><html itemscope=\"\" itemtype=\"http://schema.org/WebPage\" lang=\"ko\"><head><meta content'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.text[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!doctype html><html itemscope=\"\" itemtype=\"http://schema.org/WebPage\" lang=\"ko\"><head><meta content'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "html = requests.get('http://www.google.co.kr').text\n",
    "html[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML 소스코드를 분석하고 처리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 찾고 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><body><div><span> <a href=\"http://www.naver.com\">naver</a> <a href=\"https://www.google.com\">google</a> <a href=\"http://www.daum.net/\">daum</a> </span></div></body></html>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#테스트용 html 코드\n",
    "html =  '''<html><body><div><span>\\\n",
    "        <a href=http://www.naver.com>naver</a>\\\n",
    "        <a href=https://www.google.com>google</a>\\\n",
    "        <a href=http://www.daum.net/>daum</a>\\\n",
    "        </span></div></body></html>'''\n",
    "\n",
    "# BeautifulSoup를 이용해 HTML 소스를 파싱\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <body>\n",
      "  <div>\n",
      "   <span>\n",
      "    <a href=\"http://www.naver.com\">\n",
      "     naver\n",
      "    </a>\n",
      "    <a href=\"https://www.google.com\">\n",
      "     google\n",
      "    </a>\n",
      "    <a href=\"http://www.daum.net/\">\n",
      "     daum\n",
      "    </a>\n",
      "   </span>\n",
      "  </div>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"http://www.naver.com\">naver</a>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('a')  # 첫 번째 a태그를 찾아서 a 태그의 요소 전체를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'naver'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('a').get_text()  # a 태그의 텍스트 값만 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"http://www.naver.com\">naver</a>,\n",
       " <a href=\"https://www.google.com\">google</a>,\n",
       " <a href=\"http://www.daum.net/\">daum</a>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('a')  # 모든 a 태그를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naver\n",
      "google\n",
      "daum\n"
     ]
    }
   ],
   "source": [
    "site_names = soup.find_all('a')\n",
    "for site_name in site_names:\n",
    "    print(site_name.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 테스트용 HTML 코드\n",
    "html2 = \"\"\"\n",
    "<html>\n",
    "    <head>\n",
    "        <title>작품과 작가 모음</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>책 정보</h1>\n",
    "        <p id='book_title'>토지</p>\n",
    "        <p id='author'>박경리</p>\n",
    "        \n",
    "        <p id='book_title'>태백산맥</p>\n",
    "        <p id='author'>조정래</p>\n",
    "        \n",
    "        <p id='book_title'>감옥으로부터의 사색</p>\n",
    "        <p id='author'>신영복</p>\n",
    "    </doby>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup2 = BeautifulSoup(html2, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>작품과 작가 모음</title>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<body>\n",
       "<h1>책 정보</h1>\n",
       "<p id=\"book_title\">토지</p>\n",
       "<p id=\"author\">박경리</p>\n",
       "<p id=\"book_title\">태백산맥</p>\n",
       "<p id=\"author\">조정래</p>\n",
       "<p id=\"book_title\">감옥으로부터의 사색</p>\n",
       "<p id=\"author\">신영복</p>\n",
       "</body>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1>책 정보</h1>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.body.h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"book_title\">토지</p>,\n",
       " <p id=\"author\">박경리</p>,\n",
       " <p id=\"book_title\">태백산맥</p>,\n",
       " <p id=\"author\">조정래</p>,\n",
       " <p id=\"book_title\">감옥으로부터의 사색</p>,\n",
       " <p id=\"author\">신영복</p>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p id=\"book_title\">토지</p>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.find('p', {\"id\":'book_title'})  # p태그에서 ID가 book_title에 해당하는 가정 첫 번째 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p id=\"author\">박경리</p>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.find('p', {\"id\":'author'})# p태그에서 ID가 autor에 해당하는 가정 첫 번째 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"book_title\">토지</p>,\n",
       " <p id=\"book_title\">태백산맥</p>,\n",
       " <p id=\"book_title\">감옥으로부터의 사색</p>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.find_all('p', {\"id\":'book_title'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"author\">박경리</p>, <p id=\"author\">조정래</p>, <p id=\"author\">신영복</p>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.find_all('p', {\"id\":'author'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토지/박경리\n",
      "태백산맥/조정래\n",
      "감옥으로부터의 사색/신영복\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup2 = BeautifulSoup(html2, 'lxml')\n",
    "\n",
    "book_titles = soup2.find_all('p', {'id':\"book_title\"})\n",
    "authors = soup2.find_all('p', {'id':'author'})\n",
    "\n",
    "for book_title, author in zip(book_titles, authors):\n",
    "    print(book_title.get_text() + '/' + author.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h1>책 정보</h1>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.select('body h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"book_title\">토지</p>,\n",
       " <p id=\"author\">박경리</p>,\n",
       " <p id=\"book_title\">태백산맥</p>,\n",
       " <p id=\"author\">조정래</p>,\n",
       " <p id=\"book_title\">감옥으로부터의 사색</p>,\n",
       " <p id=\"author\">신영복</p>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.select('body p')  #body 안에p태그만 불러온다. 그냥 p를해도 나오지만 body가 여러개면?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"book_title\">토지</p>,\n",
       " <p id=\"book_title\">태백산맥</p>,\n",
       " <p id=\"book_title\">감옥으로부터의 사색</p>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.select('p#book_title')  #아까 soup2.find_all('p', {\"id\":'author'})  보다 단순한 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"author\">박경리</p>, <p id=\"author\">조정래</p>, <p id=\"author\">신영복</p>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.select('p#author')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing C:/myPyCode/HTML_example_my_site.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile C:/myPyCode/HTML_example_my_site.html \n",
    "<!doctype html>\n",
    "<html>\n",
    "  <head>\n",
    "    <meta charset=\"utf-8\">\n",
    "    <title>사이트 모음</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <p id=\"title\"><b>자주 가는 사이트 모음</b></p>\n",
    "    <p id=\"contents\">이곳은 자주 가는 사이트를 모아둔 곳입니다.</p>\n",
    "    <a href=\"http://www.naver.com\" class=\"portal\" id=\"naver\">네이버</a> <br>\n",
    "    <a href=\"https://www.google.com\" class=\"search\" id=\"google\">구글</a> <br>\n",
    "    <a href=\"http://www.daum.net\" class=\"portal\" id=\"daum\">다음</a> <br>\n",
    "    <a href=\"http://www.nl.go.kr\" class=\"government\" id=\"nl\">국립중앙도서관</a>\n",
    "  </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('C:/myPycode/HTML_example_my_site.html', encoding = 'utf-8')\n",
    "\n",
    "html3 = f.read()\n",
    "f.close()\n",
    "\n",
    "soup3 = BeautifulSoup(html3, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"portal\" href=\"http://www.naver.com\" id=\"naver\">네이버</a>,\n",
       " <a class=\"search\" href=\"https://www.google.com\" id=\"google\">구글</a>,\n",
       " <a class=\"portal\" href=\"http://www.daum.net\" id=\"daum\">다음</a>,\n",
       " <a class=\"government\" href=\"http://www.nl.go.kr\" id=\"nl\">국립중앙도서관</a>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup3.select('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"portal\" href=\"http://www.naver.com\" id=\"naver\">네이버</a>,\n",
       " <a class=\"portal\" href=\"http://www.daum.net\" id=\"daum\">다음</a>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup3.select('a.portal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"government\" href=\"http://www.nl.go.kr\" id=\"nl\">국립중앙도서관</a>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup3.select('a#nl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"contents\">이곳은 자주 가는 사이트를 모아둔 곳입니다.</p>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup3.select('p#contents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 웹 브라우저의 요소 검사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"portal\" href=\"http://www.naver.com\" id=\"naver\">네이버</a>]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup3.select('a#naver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['메일', '카페', '블로그', '지식iN', '쇼핑', 'Pay', 'TV', '사전', '뉴스', '증권', '부동산', '지도', '영화', 'VIBE', '책', '웹툰']\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#여기에 함수를 구현해봅시다.\n",
    "url = \"https://www.naver.com/\"\n",
    "req = urllib.request.urlopen(url)\n",
    "res = req.read()\n",
    "\n",
    "soup = BeautifulSoup(res,'html.parser')\n",
    "keywords = soup.find_all('',class_='nav')\n",
    "keywords = [each_line.get_text().strip() for each_line in keywords[:20]]\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing C:/myPyCode/br_example_constitution.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile C:/myPyCode/br_example_constitution.html\n",
    "<!doctype html>\n",
    "<html>\n",
    "  <head>\n",
    "    <meta charset=\"utf-8\">\n",
    "    <title>줄 바꿈 테스트 예제</title>\n",
    "  </head>\n",
    "  <body>\n",
    "  <p id=\"title\"><b>대한민국헌법</b></p>\n",
    "  <p id=\"content\">제1조 <br/>①대한민국은 민주공화국이다.<br/>②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.</p>\n",
    "  <p id=\"content\">제2조 <br/>①대한민국의 국민이 되는 요건은 법률로 정한다.<br/>②국가는 법률이 정하는 바에 의하여 재외국민을 보호할 의무를 진다.</p>\n",
    "  </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대한민국헌법\n",
      "제1조 ①대한민국은 민주공화국이다.②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.\n",
      "제2조 ①대한민국의 국민이 되는 요건은 법률로 정한다.②국가는 법률이 정하는 바에 의하여 재외국민을 보호할 의무를 진다.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "f = open('C:/myPyCode/br_example_constitution.html', encoding='utf-8')\n",
    "\n",
    "html_source = f.read()\n",
    "f.close()\n",
    "\n",
    "soup = BeautifulSoup(html_source, \"lxml\")\n",
    "\n",
    "title = soup.find('p', {\"id\":\"title\"})\n",
    "contents = soup.find_all('p', {\"id\":\"content\"})\n",
    "\n",
    "print(title.get_text())\n",
    "for content in contents:\n",
    "    print(content.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 태그 p로 찾은 요소\n",
      "<p id=\"content\">제1조 <br/>①대한민국은 민주공화국이다.<br/>②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.</p>\n",
      "==> 결과에서 태그 br로 찾은 요소: <br/>\n",
      "==> 태그 br을 개행문자로 바꾼 결과\n",
      "<p id=\"content\">제1조 \n",
      "①대한민국은 민주공화국이다.<br/>②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.</p>\n"
     ]
    }
   ],
   "source": [
    "html1 = '<p id=\"content\">제1조 <br/>①대한민국은 민주공화국이다.<br/>②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.</p>'\n",
    "\n",
    "soup1 = BeautifulSoup(html1, \"lxml\")\n",
    "\n",
    "print(\"==> 태그 p로 찾은 요소\")\n",
    "content1 = soup1.find('p', {\"id\":\"content\"})\n",
    "print(content1)\n",
    "\n",
    "br_content = content1.find(\"br\")\n",
    "print(\"==> 결과에서 태그 br로 찾은 요소:\", br_content)\n",
    "\n",
    "br_content.replace_with(\"\\n\")\n",
    "print(\"==> 태그 br을 개행문자로 바꾼 결과\")\n",
    "print(content1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p id=\"content\">제1조 \n",
      "①대한민국은 민주공화국이다.\n",
      "②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.</p>\n"
     ]
    }
   ],
   "source": [
    "soup2 = BeautifulSoup(html1, \"lxml\")\n",
    "content2 = soup2.find('p', {\"id\":\"content\"})\n",
    "\n",
    "br_contents = content2.find_all(\"br\")\n",
    "for br_content in br_contents:\n",
    "    br_content.replace_with(\"\\n\")\n",
    "print(content2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_newline(soup_html):\n",
    "    br_to_newlines = soup_html.find_all('br')\n",
    "    for br_to_newline in br_to_newlines:\n",
    "        br_to_newline.repalce_with('\\n')\n",
    "    return soup_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-7bd17b0f8830>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msoup2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lxml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcontent2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'p'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'content'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcontent3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreplace_newline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-112-c3f91226bed8>\u001b[0m in \u001b[0;36mreplace_newline\u001b[1;34m(soup_html)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mbr_to_newlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup_html\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'br'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbr_to_newline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbr_to_newlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mbr_to_newline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepalce_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msoup_html\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "soup2 = BeautifulSoup(html1, 'lxml')\n",
    "content2 = soup2.find('p', {'id': 'content'})\n",
    "content3 = replace_newline(content2)\n",
    "print(content3.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1% 성곽' 한남더힐 소유자 절반은 그곳에 살지 않았다 \n",
      "\n",
      "실거래가 기준으로 국내 최고가 아파트인 서울 용산구 한남더힐은 단 한 채만 소유해도 범상치 않은 부유함을 상징한다. 그런데 한남더힐 소유자 중에는 다른 집에 살면서 이 아파트를 ‘세컨드 하우스’(임대 등 직접 주거 이외의 목적으로 보유한 집)로 보유한 사람이 전체 가구의 절반에 육박하는 것으로 집계됐다. 이들 중에는 부자동네인 강남구나 용산구 거주자가 많았는데 이름만 들어도 알 만한 기업인과 연예인도 적지 않다. \n",
      "\n",
      "한국일보가 올해 5~9월 한남더힐의 모든 가구(600가구)의 등기부등본을 분석한 결과 소유자의 주소지가 한남더힐이 아닌 다른 곳인 사람은 모두 270명에 달했다. 이는 가구원 전부 또는 일부가 다른 곳에 살고 있다는 뜻으로, 집이 두 채 이상이거나 다른 주택에 전세 또는 월세를 살면서 한남더힐을 소유하고 있다는 의미다.  \n",
      "\n",
      "이런 비거주 소유자들은 거주지를 서울의 부촌에 둔 사람이 대부분이었다. 서울 강남구가 58명으로 가장 많았는데, 특정 아파트 단지 거주자가 도드라졌다. 압구정동 현대아파트와 한양아파트 거주자 중 13명이 한남더힐을 소유하고 있는데, 한양아파트 51동에만 한남더힐 소유자가 3명 있었다.  \n",
      "\n",
      "서울 용산구 거주자도 45명에 달했다. 이들 중 7명은 한남더힐에 거주지를 둔 채 한남더힐 단지 내에 또다른 아파트를 소유했다. 예컨대, 의류제조업체를 운영하는 기업인 일가의 경우 70대 어머니와 40대 자녀 2명(아들, 딸)은 한남더힐 127동 한 채를 공동 소유하고 있다. 그런데 어머니는 한남더힐 112동에 월세로 거주 중이며, 딸은 한남더힐 124동 한 채를 단독 소유하고 있다. 아들은 경기 양평군에 산다. \n",
      "\n",
      "이밖에 서울 서초구 거주자가 35명, 성동구 13명, 종로구 10명, 송파구 8명 등으로 나타났다. 서울 이외 지역에 거주지를 둔 채 한남더힐을 가진 사람은 부산 7명, 대구 6명, 대전 6명, 광주ㆍ제주 4명, 충북 2명 등이다. 거주지가 일본 도쿄인  소유자도 3명(모두 한국인) 있었다. \n",
      "\n",
      "유명인사 가운데에도, 한남더힐을 소유했지만 등기부등본상 주소지가 다른 곳인 사례(올해 5~9월 조사 기준)가 드물지 않았다. 재벌가 중에선 구광모 LG그룹 회장의 주소지가 서울 한남동 저택이며, 정의선 현대차 회장의 누나인 정성이 이노션 고문 또한 주소지가 한남동 저택이다. 김택진 엔씨소프트 대표는 주소지가 강남구 삼성동 아파트이고, 김성수 카카오M 대표는 압구정동 아파트에 주소지를 뒀다. 한남더힐 소유자인 삼성전자의 김기남 부회장은 강남구 대치동에, 삼성디스플레이 이동훈 사장은 경기 용인시에 각각 주소지가 있다. 안병광 유니온약품그룹 회장은 본인이 소유한 조선시대 문화유적인 종로구 부암동 ‘석파정’에 주소지를 두고 있는 것으로 나타났다. \n",
      "\n",
      "최근 아모레퍼시픽 서경배 회장의 맏딸과 결혼한, 홍석준 보광창업투자 회장의 장남 정환씨는 주소지가 성북동 저택이다. 최신원 SK네트웍스 회장의 장남인 최성환 SK네트웍스 전략기획실장은 인근 이촌동 아파트에 주소지가 있고, 구자철 예스코홀딩스 회장은 주소지가 장충동이었다. 유경선 유진그룹 회장의 부인 구금숙씨는 한남동 저택에 주소를 뒀다. 부부 공동명의로 한남더힐을 소유했지만 전세를 내준 박성훈 부산시 경제부시장은 주소지가 서울시 중구 회현동 아파트로 돼 있었다.  \n",
      "\n",
      "연예인 중에서도 세컨드하우스 개념으로 한남더힐을 소유한 것으로 추정되는 사람들이 있었다. 배우 김태희는 용산구 이태원동에, 소지섭은 한남동 유엔빌리지에 각각 주소지가 있었고, 한효주와 이요원은 주소지가 강남구 청담동이다. 역시 한남더힐 소유주인 배우 추자현과 드라마 ‘도깨비’를 쓴 김은숙 작가는 둘 다 주소지가 경기 고양시 일산이다. SBS 드라마 '하늘이시여'에 나온 배우 윤정희는 서울 서초구 잠원동 아파트에 주소가 있었다. \n",
      "\n",
      "별도 주소지를 둔 소유자가 유독 많은 이런 특징에 대해 김남근 법무법인 위민 변호사는 \"고가 아파트일수록 실거주보다는 투기 목적으로 구입한 사람들이 많다는 걸 보여주는 것\"이라고 해석했다 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = requests.get('https://www.hankookilbo.com/News/Read/A2020101618490003127')\n",
    "html = url.text\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "title = soup.find('h2',{'class':'title'})\n",
    "news = soup.select('p.editor-p')\n",
    "\n",
    "print(title.get_text(),'\\n')\n",
    "for content in news:\n",
    "    print(content.get_text(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
