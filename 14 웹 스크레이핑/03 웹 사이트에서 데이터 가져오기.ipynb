{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.alexa.com/topsites/countries/KR'\n",
    "\n",
    "html_website_ranking = requests.get(url).text\n",
    "soup_website_ranking = BeautifulSoup(html_website_ranking, 'lxml')\n",
    "\n",
    "# p 태그의 요소 안에서 a 태그의 요소를 찾음\n",
    "website_ranking = soup_website_ranking.select('p a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Google.com'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_ranking[1].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_ranking_adress = [website_ranking_element.get_text() for website_ranking_element in \n",
    "                         website_ranking]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Google.com',\n",
       " 'Naver.com',\n",
       " 'Youtube.com',\n",
       " 'Daum.net',\n",
       " 'Tistory.com',\n",
       " 'Tmall.com']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_ranking_adress[1:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "탑뮤직\n",
      "1: Dynamite\n",
      "2: Lovesick Girls\n",
      "3: DON'T TOUCH ME\n",
      "4: 잠이 오질 않네요\n",
      "5: 취기를 빌려 (취향저격 그녀 X 산들)\n",
      "6: 힘든 건 사랑이 아니다\n",
      "7: I CAN'T STOP ME\n",
      "8: Savage Love (Laxed - Siren Beat) (BTS Remix)\n",
      "9: 딩가딩가 (Dingga)\n",
      "10: 마리아 (Maria)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://music.naver.com/listen/top100.nhn?domain=TOTAL_V2'\n",
    "\n",
    "html = requests.get(url).text\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "rank = soup.select('a._title')\n",
    "adress = [ranking.get_text() for ranking in rank]\n",
    "\n",
    "print(\"탑뮤직\")\n",
    "for k in range(10):\n",
    "    print(f'{k+1}: {adress[k]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[긴박엔딩] 이지아, 김소연×엄기준 불륜 현장 목격 후 들키기 ‘일보 직전!’\n",
      "잠시만 안녕. 김호중 군복무전 마지막 무대 ＜배웅＞ㅣ김호중의파트너 EP.5\n",
      "임신 확률 겨우 4%😭 둘째 원하는 이유!?\n",
      "임영웅 ‘옛사랑’ ♪ 감성 장인 히어로 💧\n",
      "[키스 엔딩] ＂보고 싶었어...＂ 김하늘♥이도현의 변하지 않은 사랑\n",
      "[선공개] 주저앉은 김범! 이동욱과 아귀의 숲 탈출 포기?\n",
      "이지아, 이철민이 말한 친딸 관련된 진실에 ‘오열’ (ft. 엄기준 소름)\n",
      "17살 차이나는 박휘순♥천예지 부부!! 나이보단 사랑이 우선이다..☆\n",
      "이지아, 진짜 친딸 안 뒤 조수민이 당한 과거 회상에 ‘눈물’\n",
      "“후식은 중국집” 김호중×현주엽, 끝나지 않는 먹레이스!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://tv.naver.com/r/'\n",
    "\n",
    "html = requests.get(url).text\n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "titles = soup.select('dt.title')\n",
    "rank = [title.get_text() for title in titles]\n",
    "for i in rank[0:10]:\n",
    "    print(i.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "연도를 입력하세요 2020\n",
      "월을 입력하세요 3\n",
      "주를 입력하세요 2\n",
      "METEOR\n",
      "Psycho\n",
      "Blueming\n",
      "HIP\n",
      "Square (2017)\n",
      "어떻게 이별까지 사랑하겠어, 널 사랑하는 거지\n",
      "Love poem\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "year = input('연도를 입력하세요 ')\n",
    "month = input('월을 입력하세요 ')\n",
    "week = input('주를 입력하세요 ')\n",
    "\n",
    "if len(month) == 1: month = '0'+month\n",
    "\n",
    "url = f\"http://music.naver.com/listen/history/index.nhn?type=TOTAL&year={year}&month={month}&week={week}\"\n",
    "html_music = requests.get(url).text\n",
    "soup_music = BeautifulSoup(html_music, \"lxml\")\n",
    "\n",
    "# a 태그의 요소 중에서 class 속성값이 \"_title\" 인 것을 찾고\n",
    "# 그 안에서 span 태그의 요소 중에서 class 속성값이 \"ellipsis\"인 요소를 추출\n",
    "titles = soup_music.select('a._title span.ellipsis') \n",
    "\n",
    "\n",
    "for title in titles[0:7]:\n",
    "    print(title.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t창모(CHANGMO)\\r\\n\\t\\t\\n'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a 태그의 요소 중에서 class 속성값이 \"_artist\" 인 것을 찾고\n",
    "# 그 안에서 span 태그의 요소 중에서 class 속성값이 \"ellipsis\"인 요소를 추출\n",
    "\n",
    "artists = soup_music.select('td._artist a')\n",
    "artists[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "창모(CHANGMO)\n",
      "Red Velvet (레드벨벳)\n",
      "아이유(IU)\n",
      "마마무(Mamamoo)\n",
      "백예린\n",
      "AKMU (악동뮤지션)\n",
      "아이유(IU)\n"
     ]
    }
   ],
   "source": [
    "music_artists = [artist.get_text().strip() for artist in artists]\n",
    "music_artists[0:7]\n",
    "for music_artist in music_artists[0:7]:\n",
    "    print(music_artist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "연도를 입력하세요 2020\n",
      "월을 입력하세요 3\n",
      "주를 입력하세요 1\n",
      "1.METEOR - 창모(CHANGMO)\n",
      "2.Psycho - Red Velvet (레드벨벳)\n",
      "3.Blueming - 아이유(IU)\n",
      "4.HIP - 마마무(Mamamoo)\n",
      "5.Square (2017) - 백예린\n",
      "6.어떻게 이별까지 사랑하겠어, 널 사랑하는 거지 - AKMU (악동뮤지션)\n",
      "7.Love poem - 아이유(IU)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "year = input('연도를 입력하세요 ')\n",
    "month = input('월을 입력하세요 ')\n",
    "week = input('주를 입력하세요 ')\n",
    "\n",
    "if len(month) == 1: month = '0'+month\n",
    "\n",
    "url = f\"http://music.naver.com/listen/history/index.nhn?type=TOTAL&year={year}&month={month}&week={week}\"\n",
    "html_music = requests.get(url).text\n",
    "soup_music = BeautifulSoup(html_music, \"lxml\")\n",
    "\n",
    "# a 태그의 요소 중에서 class 속성값이 \"_title\" 인 것을 찾고\n",
    "# 그 안에서 span 태그의 요소 중에서 class 속성값이 \"ellipsis\"인 요소를 추출\n",
    "titles = soup_music.select('a._title span.ellipsis') \n",
    "\n",
    "# a 태그의 요소 중에서 class 속성값이 \"_artist\" 인 것을 찾고\n",
    "# 그 안에서 span 태그의 요소 중에서 class 속성값이 \"ellipsis\"인 요소를 추출\n",
    "\n",
    "artists = soup_music.select('td._artist a')\n",
    "\n",
    "for k in range(7):\n",
    "    print(f'{k+1}.{titles[k].get_text().strip()} - {artists[k].get_text().strip()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/myPyCode/data/NaverMusicTop100.xlsx']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import glob\n",
    "\n",
    "naver_music_url = \"http://music.naver.com/listen/history/index.nhn?type=DOMESTIC&year=2017&month=12&week=1&page=\"\n",
    " \n",
    "# 네이버 music 주소를 입력하면 노래 제목과 아티스트를 반환\n",
    "def naver_music(url):    \n",
    "    html_music = requests.get(url).text\n",
    "    soup_music = BeautifulSoup(html_music, \"lxml\")\n",
    "\n",
    "    titles = soup_music.select('a._title span.ellipsis') \n",
    "    artists = soup_music.select('td._artist a')\n",
    "\n",
    "    music_titles = [title.get_text() for title in titles]\n",
    "    music_artists = [artist.get_text().strip() for artist in artists]\n",
    "    \n",
    "    return music_titles, music_artists\n",
    "\n",
    "# 노래 제목과 아티스트를 저장할 파일 이름을 폴더와 함께 지정\n",
    "file_name = 'C:/myPyCode/data/NaverMusicTop100.txt'\n",
    "\n",
    "f = open(file_name,'w') # 파일 열기\n",
    "\n",
    "# 각 page에는 50개의 노래 제목과 아티스트가 추출됨\n",
    "for page in range(2):\n",
    "    naver_music_url_page = naver_music_url + str(page+1) # page URL\n",
    "    naver_music_titles, naver_music_artists = naver_music(naver_music_url_page)\n",
    "    \n",
    "    # 추출된 노래 제목과 아티스트를 파일에 저장 \n",
    "    for k in range(len(naver_music_titles)):\n",
    "        f.write(\"{0:2d}: {1}/{2}\\n\".format(page*50 + k+1, naver_music_titles[k],  naver_music_artists[k]))\n",
    "        \n",
    "f.close() # 파일 닫기\n",
    "glob.glob(file_name) # 생성된 파일 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HERO', '임영웅']\n",
      "['호랑이는 죽어서 가죽을, 우리는 이름을', '홈보이 (Homeboy), 에네이 에미 (Enei Emi)']\n",
      "['떨어져 (Please Go Away)', 'NIDA']\n",
      "['싸이코4U', '걸카인드 (GIRLKIND)']\n",
      "['AYA', '마마무 (Mamamoo)']\n",
      "['SOON (Feat. BewhY)', '다이나믹 듀오']\n",
      "['Bangkok (Stay Home)', '뷰티핸섬']\n",
      "['By your side (Feat. E.N.D., SLOC)', 'TSLW']\n",
      "['어쩌면 오래전부터', '캡틴플래닛, 이은아']\n",
      "['Winter again', 'Yolk']\n",
      "['얘랑 있을 때 좋다(with 김국헌 of 비오브유)', '어쿠스틱 콜라보']\n",
      "[\"I'm alone\", 'The Hills (더 힐스)']\n",
      "['Wind, Star (feat. 이상훈 of 훈스 (HOONS))', '릴리노트 (Lilynote)']\n",
      "['백일홍', '조명섭']\n",
      "['잔상 (Vocal. 정구영)', '김현수 (F-IV)']\n",
      "['귀가 (With 타미즈)', '42kgb']\n",
      "['너라는 하루를 살아', '주호']\n",
      "['그때 생각나?(Remember that time?)', '베이비블루(BABY BLUE)']\n",
      "['i got you', 'Jimmy Brown']\n",
      "['On The Radio (With SERO)', 'SUDI']\n",
      "['서울', '임예송']\n",
      "['Love Killa', '몬스타엑스']\n",
      "['SALUTE', 'AB6IX (에이비식스)']\n",
      "['비행기 모드 (Feat. 바끼리, BRADYSTREET)', '나왔구나 김비노']\n",
      "['안돼요', '홍진영']\n",
      "['G.B.T.B. (Strong Dragon Remix)', 'VERIVERY']\n",
      "['OFF', '새벽두시']\n",
      "['I need you. Period.', 'Kim Jeong_uk(김정욱)']\n",
      "['좋아 좋아', '정다경']\n",
      "['강을 향해 간다', '스마일리스마일 (SmileySmile)']\n",
      "['INTRCRSE (Feat. 바끼리)', 'DOPA']\n",
      "['Fxxkin Insomnia', 'MÖA']\n",
      "['소년', '신지민']\n",
      "['MOVIE (Feat. Rohann)', '정진우']\n",
      "['텅 비었던(ft. 서한)', 'Xbf']\n",
      "[\"DIVIN' (Feat. z4vwm (잡음), JUICY KID)\", '온리차일드 (ONLICHILD)']\n",
      "['그대 그리운 날', '설가령']\n",
      "['Love you Love you (feat.더 브릿지(The Bridge))', '브런치 레시피 (Brunch recipe)']\n",
      "['Labyrinth', '정주 (Jungju)']\n",
      "['Be bop a lula', '얼스 (Earls), 이철호']\n",
      "['Lazy Seoul (레이지 서울)', '제이켠']\n",
      "['온도차', '화자 (HWAJA)']\n",
      "['내버려둬(NOYB)', 'BXK']\n",
      "['태양', '이준형']\n",
      "['Overdose (feat. Tommy Strate)', 'Claire Hau (클레어 하우)']\n",
      "['My Love', '다비치']\n",
      "['우리 멀리 떠날까 (feat. 정기고)', '고닥']\n",
      "['minimal warm (취향저격 그녀 X 찬열 (CHANYEOL))', '찬열 (CHANYEOL)']\n",
      "['My Universe (Prod. by Minit)', 'Holynn']\n",
      "['Traveler (feat. Zooy, Vibin)', 'TAETAE']\n",
      "['HERO', '임영웅']\n",
      "['호랑이는 죽어서 가죽을, 우리는 이름을', '홈보이 (Homeboy), 에네이 에미 (Enei Emi)']\n",
      "['떨어져 (Please Go Away)', 'NIDA']\n",
      "['싸이코4U', '걸카인드 (GIRLKIND)']\n",
      "['AYA', '마마무 (Mamamoo)']\n",
      "['SOON (Feat. BewhY)', '다이나믹 듀오']\n",
      "['Bangkok (Stay Home)', '뷰티핸섬']\n",
      "['By your side (Feat. E.N.D., SLOC)', 'TSLW']\n",
      "['어쩌면 오래전부터', '캡틴플래닛, 이은아']\n",
      "['Winter again', 'Yolk']\n",
      "['얘랑 있을 때 좋다(with 김국헌 of 비오브유)', '어쿠스틱 콜라보']\n",
      "[\"I'm alone\", 'The Hills (더 힐스)']\n",
      "['Wind, Star (feat. 이상훈 of 훈스 (HOONS))', '릴리노트 (Lilynote)']\n",
      "['백일홍', '조명섭']\n",
      "['잔상 (Vocal. 정구영)', '김현수 (F-IV)']\n",
      "['귀가 (With 타미즈)', '42kgb']\n",
      "['너라는 하루를 살아', '주호']\n",
      "['그때 생각나?(Remember that time?)', '베이비블루(BABY BLUE)']\n",
      "['i got you', 'Jimmy Brown']\n",
      "['On The Radio (With SERO)', 'SUDI']\n",
      "['서울', '임예송']\n",
      "['Love Killa', '몬스타엑스']\n",
      "['SALUTE', 'AB6IX (에이비식스)']\n",
      "['비행기 모드 (Feat. 바끼리, BRADYSTREET)', '나왔구나 김비노']\n",
      "['안돼요', '홍진영']\n",
      "['G.B.T.B. (Strong Dragon Remix)', 'VERIVERY']\n",
      "['OFF', '새벽두시']\n",
      "['I need you. Period.', 'Kim Jeong_uk(김정욱)']\n",
      "['좋아 좋아', '정다경']\n",
      "['강을 향해 간다', '스마일리스마일 (SmileySmile)']\n",
      "['INTRCRSE (Feat. 바끼리)', 'DOPA']\n",
      "['Fxxkin Insomnia', 'MÖA']\n",
      "['소년', '신지민']\n",
      "['MOVIE (Feat. Rohann)', '정진우']\n",
      "['텅 비었던(ft. 서한)', 'Xbf']\n",
      "[\"DIVIN' (Feat. z4vwm (잡음), JUICY KID)\", '온리차일드 (ONLICHILD)']\n",
      "['그대 그리운 날', '설가령']\n",
      "['Love you Love you (feat.더 브릿지(The Bridge))', '브런치 레시피 (Brunch recipe)']\n",
      "['Labyrinth', '정주 (Jungju)']\n",
      "['Be bop a lula', '얼스 (Earls), 이철호']\n",
      "['Lazy Seoul (레이지 서울)', '제이켠']\n",
      "['온도차', '화자 (HWAJA)']\n",
      "['내버려둬(NOYB)', 'BXK']\n",
      "['태양', '이준형']\n",
      "['Overdose (feat. Tommy Strate)', 'Claire Hau (클레어 하우)']\n",
      "['My Love', '다비치']\n",
      "['우리 멀리 떠날까 (feat. 정기고)', '고닥']\n",
      "['minimal warm (취향저격 그녀 X 찬열 (CHANYEOL))', '찬열 (CHANYEOL)']\n",
      "['My Universe (Prod. by Minit)', 'Holynn']\n",
      "['Traveler (feat. Zooy, Vibin)', 'TAETAE']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import openpyxl\n",
    "\n",
    "wb = openpyxl.Workbook()\n",
    "sheet = wb.active\n",
    "sheet.append([\"제목\", \"가수\"])\n",
    "\n",
    "\n",
    "naver_music_url = \"http://music.naver.com/listen/history/index.nhn?type=DOMESTIC&year=2017&month=12&week=1&page=\"\n",
    "\n",
    "def naver_music(url):    \n",
    "    html_music = requests.get(url).text\n",
    "    soup_music = BeautifulSoup(html_music, \"lxml\")\n",
    "\n",
    "    titles = soup_music.select('a._title span.ellipsis') \n",
    "    artists = soup_music.select('td._artist a')\n",
    "\n",
    "    music_titles = [title.get_text() for title in titles]\n",
    "    music_artists = [artist.get_text() for artist in artists]\n",
    "    return music_titles, music_artists\n",
    "\n",
    "\n",
    "for page in range(2):\n",
    "    naver_music_url_page = naver_music_url + str(page+1) # page URL\n",
    "    naver_music_titles, naver_music_artists = naver_music(naver_music_url_page)\n",
    "    \n",
    "    for k in range(len(naver_music_titles)):\n",
    "        sheet.append([music_titles[k].strip(), music_artists[k].strip()])\n",
    "        print([music_titles[k].strip(), music_artists[k].strip()])\n",
    "wb.save('C:/myPycode/data/fuckyou.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamite - 방탄소년단\n",
      "DON'T TOUCH ME - 환불원정대\n",
      "Lovesick Girls - BLACKPINK\n",
      "힘든 건 사랑이 아니다 - 임창정\n",
      "취기를 빌려 (취향저격 그녀 X 산들) - 산들\n",
      "오래된 노래 - 스탠딩 에그\n",
      "Savage Love (Laxed - Siren Beat) (BTS Remix) - Jawsh 685, Jason Derulo, 방탄소년단\n",
      "When We Disco (Duet with 선미) - 박진영\n",
      "놓아줘 (with 태연) - Crush\n",
      "내 마음이 움찔했던 순간 (취향저격 그녀 X 규현) - 규현 (KYUHYUN)\n",
      "잠이 오질 않네요 - 장범준\n",
      "눈누난나 (NUNU NANA) - 제시 (Jessi)\n",
      "마리아 (Maria) - 화사 (Hwa Sa)\n",
      "How You Like That - BLACKPINK\n",
      "에잇(Prod.&Feat. SUGA of BTS) - 아이유\n",
      "흔들리는 꽃들 속에서 네 샴푸향이 느껴진거야 - 장범준\n",
      "딩가딩가 (Dingga) - 마마무 (Mamamoo)\n",
      "Dolphin - 오마이걸 (OH MY GIRL)\n",
      "아로하 - 조정석\n",
      "어떻게 지내 (Prod. By VAN.C) - 오반\n",
      "다시 여기 바닷가 - 싹쓰리 (유두래곤, 린다G, 비룡)\n",
      "늦은 밤 너의 집 앞 골목길에서 - 노을\n",
      "홀로 - 이하이\n",
      "I CAN’T STOP ME - TWICE (트와이스)\n",
      "Downtown Baby - 블루 (BLOO)\n",
      "Dance Monkey - Tones And I\n",
      "Blueming - 아이유\n",
      "어떻게 이별까지 사랑하겠어, 널 사랑하는 거지 - AKMU (악동뮤지션)\n",
      "METEOR - 창모 (CHANGMO)\n",
      "Memories - Maroon 5\n",
      "거짓말이라도 해서 널 보고싶어 - 백지영\n",
      "모든 날, 모든 순간 (Every day, Every Moment) - 폴킴\n",
      "사랑은 지날수록 더욱 선명하게 남아 - 전상근\n",
      "작은 것들을 위한 시 (Boy With Luv) (Feat. Halsey) - 방탄소년단\n",
      "살짝 설렜어 (Nonstop) - 오마이걸 (OH MY GIRL)\n",
      "서면역에서 - 순순희\n",
      "Bad Boy - 청하, Christopher\n",
      "오늘도 빛나는 너에게 (To You My Light) (Feat.이라온) - 마크툽 (MAKTUB)\n",
      "너도 아는 - 폴킴\n",
      "2002 - Anne-Marie\n",
      "우리 왜 헤어져야 해 - 신예영\n",
      "사랑하게 될 줄 알았어 - 전미도\n",
      "마음을 드려요 - 아이유\n",
      "Don't Start Now - Dua Lipa\n",
      "덤디덤디 (DUMDi DUMDi) - (여자)아이들\n",
      "처음처럼 - 엠씨더맥스 (M.C the MAX)\n",
      "Ice Cream (with Selena Gomez) - BLACKPINK\n",
      "사랑 못해, 남들 쉽게 다 하는 거 - 먼데이 키즈 (Monday Kiz)\n",
      "나랑 같이 걸을래 (바른연애 길잡이 X 적재) - 적재\n",
      "Summer Hate (Feat. 비) - 지코 (ZICO)\n",
      "Not Shy - ITZY (있지)\n",
      "봄날 - 방탄소년단\n",
      "안녕 - 폴킴\n",
      "Pretty Savage - BLACKPINK\n",
      "아무노래 - 지코 (ZICO)\n",
      "보라빛 밤 (pporappippam) - 선미\n",
      "12:45 (Stripped) - Etham\n",
      "시작 - 가호 (Gaho)\n",
      "가을 타나 봐 - 바이브\n",
      "ON - 방탄소년단\n",
      "Maniac - Conan Gray\n",
      "이제 나만 믿어요 - 임영웅\n",
      "잠이 들어야 (Feat. 헤이즈) - 로꼬\n",
      "Love poem - 아이유\n",
      "너를 만나 - 폴킴\n",
      "좋은 사람 있으면 소개시켜줘 - 조이 (JOY)\n",
      "Paris In The Rain - Lauv\n",
      "밤새 (취향저격 그녀 X 카더가든) - 카더가든\n",
      "Into the I-LAND - 아이유\n",
      "축하해 - 오반\n",
      "취했나봐 - 황인욱\n",
      "그때 그 아인 - 김필\n",
      "Psycho - Red Velvet (레드벨벳)\n",
      "별을 담은 시 (Ode To The Stars) - 마크툽 (MAKTUB), 이라온\n",
      "행복해 - 송하예\n",
      "시든 꽃에 물을 주듯 - HYNN (박혜원)\n",
      "ALIEN - 이수현\n",
      "bad guy - Billie Eilish\n",
      "그 여름을 틀어줘 - 싹쓰리 (유두래곤, 린다G, 비룡)\n",
      "Painkiller - Ruel\n",
      "나비와 고양이 (feat.백현 (BAEKHYUN)) - 볼빨간사춘기\n",
      "하루도 그대를 사랑하지 않은 적이 없었다 - 임창정\n",
      "행복하니 - 케이시 (Kassy)\n",
      "소확행 - 임창정\n",
      "Bet You Wanna (Feat. Cardi B) - BLACKPINK\n",
      "Stuck with U - Ariana Grande, Justin Bieber\n",
      "반만 - 진민호\n",
      "사랑하고 싶지 않아 (바른연애 길잡이 X XIA (준수)) - 김준수\n",
      "첫 줄 - 신용재 (2F)\n",
      "화려하지 않은 고백 - 규현 (KYUHYUN)\n",
      "사랑이란 멜로는 없어 - 전상근\n",
      "Tip Toe (with 이하이) - Crush\n",
      "밤하늘의 저 별처럼 - 헤이즈 (Heize), 펀치 (Punch)\n",
      "신난다 (Feat. 마마무) - 비룡\n",
      "너를 사랑하고 있어 - 백현 (BAEKHYUN)\n",
      "HIP - 마마무 (Mamamoo)\n",
      "MORE & MORE - TWICE (트와이스)\n",
      "To Die For - Sam Smith\n",
      "00:00 (Zero O’Clock) - 방탄소년단\n",
      "너의 밤은 어때 (취향저격 그녀 X 정은지) - 정은지\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import openpyxl\n",
    "\n",
    "wb = openpyxl.Workbook()\n",
    "sheet = wb.active\n",
    "sheet.append(['순위', \"제목\", \"가수\"])\n",
    "\n",
    "\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'}\n",
    "\n",
    "url = 'https://www.melon.com/chart/week/index.htm'\n",
    "\n",
    "html_music = requests.get(url, headers = headers).text\n",
    "soup_music = BeautifulSoup(html_music, \"lxml\")\n",
    "\n",
    "titles = soup_music.select('td div.ellipsis.rank01') \n",
    "artists = soup_music.select('td div.ellipsis.rank02 span') \n",
    "\n",
    "music_titles = [title.get_text() for title in titles]\n",
    "music_artists = [artist.get_text() for artist in artists]\n",
    "\n",
    "for k in range(len(music_titles)):\n",
    "    sheet.append([k+1, music_titles[k].strip(), music_artists[k].strip()])\n",
    "    print(music_titles[k].strip(), '-', music_artists[k].strip())\n",
    "wb.save('C:/myPycode/data/fuckyoumelon.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-5caa8467b724>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'c:/chromedriver'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimplicitly_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "driver = webdriver.Chrome('c:/chromedriver')\n",
    "driver.implicitly_wait(3)\n",
    "driver.get('http://music.naver.com/listen/history/index.nhn?type=DOMESTIC&year=2017&month=12&week=1&page=1')\n",
    "\n",
    "driver.find_element_by_xpath('//*[@id=\"content\"]/div[3]/div[2]/div[1]/table/tbody/tr[6]/td[5]/a').click()\n",
    "html = driver.page_source\n",
    "\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "result = soup.select('div.scrollbar-content')\n",
    "\n",
    "artists = result[3].get_text()\n",
    "artists_first = artists[0:3]\n",
    "artists_second = artists[3:]\n",
    "\n",
    "print(artists_first+\", \"+ artists_second)\n",
    "\n",
    "rank5 = artists_first+\", \"+ artists_second\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 웹페이지에서 이미지 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests  \n",
    "\n",
    "url = 'https://www.python.org/static/img/python-logo.png'\n",
    "html_image = requests.get(url)\n",
    "html_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python-logo.png'"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "image_file_name = os.path.basename(url)\n",
    "image_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'C:/myPyCode/download' \n",
    "\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/myPyCode/download\\\\python-logo.png'"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = os.path.join(folder, image_file_name)\n",
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageFile = open(image_path, 'wb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 데이터를 1000000 바이트씩 나눠서 내려받고 파일에 순차적으로 저장\n",
    "chunk_size = 1000000\n",
    "for chunk in html_image.iter_content(chunk_size):\n",
    "    imageFile.write(chunk)\n",
    "imageFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['python-logo.png']"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  \n",
    "import os\n",
    "\n",
    "url = 'https://www.python.org/static/img/python-logo.png'\n",
    "html_image = requests.get(url)\n",
    "image_file_name = os.path.basename(url)\n",
    "\n",
    "folder = 'C:/myPyCode/download' \n",
    "\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "image_path = os.path.join(folder, image_file_name)\n",
    "\n",
    "imageFile = open(image_path, 'wb')\n",
    "# 이미지 데이터를 1000000 바이트씩 나눠서 저장\n",
    "chunk_size = 1000000\n",
    "for chunk in html_image.iter_content(chunk_size):\n",
    "    imageFile.write(chunk)\n",
    "imageFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  \n",
    "import os\n",
    "\n",
    "url = 'https://imgnews.pstatic.net/image/011/2020/10/31/0003818880_001_20201031180544196.jpg'\n",
    "html_image = requests.get(url)\n",
    "image_file_name = os.path.basename(url)\n",
    "\n",
    "folder = 'C:/myPyCode/download' \n",
    "\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "image_path = os.path.join(folder, image_file_name)\n",
    "\n",
    "imageFile = open(image_path, 'wb')\n",
    "# 이미지 데이터를 1000000 바이트씩 나눠서 저장\n",
    "chunk_size = 1000000\n",
    "for chunk in html_image.iter_content(chunk_size):\n",
    "    imageFile.write(chunk)\n",
    "imageFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 여러 이미지 내려받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<img alt=\"Reshot navbar logo\" class=\"black\" src=\"https://assets-static.reshot-cdn.com/brand/reshot-logo.png\"/>,\n",
       " <img alt=\"Free authentic animals photo on Reshot\" src=\"https://res.cloudinary.com/twenty20/private_images/t_standard-fit/v1521838685/photosp/bae96789-a5ab-4471-b54f-9686ace09e33/bae96789-a5ab-4471-b54f-9686ace09e33.jpg\"/>,\n",
       " <img alt=\"Back off!\" src=\"https://res.cloudinary.com/twenty20/private_images/t_standard-fit/v1597098233/photosp/a44357c5-b1c3-41ef-9a65-7a4937b06a44/a44357c5-b1c3-41ef-9a65-7a4937b06a44.jpg\"/>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.reshot.com/search/animals'\n",
    "\n",
    "html_reshot_image = requests.get(url).text\n",
    "\n",
    "soup_reshot_image = BeautifulSoup(html_reshot_image, 'lxml')\n",
    "\n",
    "\n",
    "reshot_image_elements = soup_reshot_image.select('a img')\n",
    "\n",
    "reshot_image_elements[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://res.cloudinary.com/twenty20/private_images/t_standard-fit/v1521838685/photosp/bae96789-a5ab-4471-b54f-9686ace09e33/bae96789-a5ab-4471-b54f-9686ace09e33.jpg'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshot_image_url = reshot_image_elements[1].get('src')\n",
    "reshot_image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_image = requests.get(reshot_image_url)\n",
    "\n",
    "folder = \"C:/myPyCode/download\"\n",
    "    \n",
    "# os.path.basename(URL)는 웹사이트나 폴더가 포함된 파일명에서 파일명만 분리하는 방법    \n",
    "imageFile = open(os.path.join(folder, os.path.basename(reshot_image_url)), 'wb')\n",
    "\n",
    "# 이미지 데이터를 1000000 바이트씩 나눠서 저장하는 방법\n",
    "chunk_size = 1000000 \n",
    "for chunk in html_image.iter_content(chunk_size):\n",
    "    imageFile.write(chunk)\n",
    "imageFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MissingSchema",
     "evalue": "Invalid URL '//search1.daumcdn.net/search/statics/common/pi/logo/daumlogo_170324.png': No schema supplied. Perhaps you meant http:////search1.daumcdn.net/search/statics/common/pi/logo/daumlogo_170324.png?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMissingSchema\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-26768a6018fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_of_download_image\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mdownload_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigure_folder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpixabay_image_urls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"================================\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"선택한 모든 이미지 내려받기 완료!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-26768a6018fd>\u001b[0m in \u001b[0;36mdownload_image\u001b[1;34m(img_folder, img_url)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdownload_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_url\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mhtml_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[1;31m# os.path.basename(URL)는 웹사이트나 폴더가 포함된 파일명에서 파일명만 분리\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mimageFile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    514\u001b[0m             \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m         )\n\u001b[1;32m--> 516\u001b[1;33m         \u001b[0mprep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[0mproxies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproxies\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mprepare_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPreparedRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m         p.prepare(\n\u001b[0m\u001b[0;32m    450\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m             \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_headers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_cookies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcookies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mprepare_url\u001b[1;34m(self, url, params)\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_native_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mMissingSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhost\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMissingSchema\u001b[0m: Invalid URL '//search1.daumcdn.net/search/statics/common/pi/logo/daumlogo_170324.png': No schema supplied. Perhaps you meant http:////search1.daumcdn.net/search/statics/common/pi/logo/daumlogo_170324.png?"
     ]
    }
   ],
   "source": [
    "import requests  \n",
    "from bs4 import BeautifulSoup \n",
    "import os\n",
    "\n",
    "# URL(주소)에서 이미지 주소 추출\n",
    "def get_image_url(url): \n",
    "    html_image_url = requests.get(url).text\n",
    "    soup_image_url = BeautifulSoup(html_image_url, \"lxml\")  \n",
    "    image_elements = soup_image_url.select('a img') \n",
    "    if(image_elements != None):\n",
    "        image_urls = []\n",
    "        for image_element in image_elements:\n",
    "            image_urls.append(image_element.get('src'))\n",
    "        return image_urls        \n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "# 폴더를 지정해 이미지 주소에서 이미지 내려받기\n",
    "def download_image(img_folder, img_url):\n",
    "    if(img_url != None):  \n",
    "        html_image = requests.get(img_url)\n",
    "        # os.path.basename(URL)는 웹사이트나 폴더가 포함된 파일명에서 파일명만 분리    \n",
    "        imageFile = open(os.path.join(img_folder, os.path.basename(img_url)), 'wb')\n",
    "\n",
    "        chunk_size = 1000000 # 이미지 데이터를 1000000 바이트씩 나눠서 저장\n",
    "        for chunk in html_image.iter_content(chunk_size):\n",
    "            imageFile.write(chunk)\n",
    "            imageFile.close()\n",
    "        print(\"이미지 파일명: '{0}'. 내려받기 완료!\".format(os.path.basename(img_url))) \n",
    "    else:       \n",
    "        print(\"내려받을 이미지가 없습니다.\")\n",
    "        \n",
    "# 웹 사이트의 주소 지정   \n",
    "pixabay_url = 'https://search.daum.net/search?w=img&nil_search=btn&DA=NTB&enc=utf8&q=%ED%8A%B8%EB%9F%BC%ED%94%84'\n",
    "# pixabay_url= 'https://pixabay.com/ko/photos/?order=popular&cat=animals&pagi=2'\n",
    "\n",
    "figure_folder = \"C:/myPyCode/download/trump\" # 이미지를 내려받을 폴더 지정  \n",
    "if not os.path.exists(figure_folder):\n",
    "    os.makedirs(figure_folder)\n",
    "    \n",
    "pixabay_image_urls = get_image_url(pixabay_url) # 이미지 파일의 주소 가져오기\n",
    "\n",
    "num_of_download_image = 7 # 내려받을 이미지 개수 지정\n",
    "# num_of_download_image = len(pixabay_image_urls)\n",
    "\n",
    "for k in range(num_of_download_image+1):\n",
    "    download_image(figure_folder,pixabay_image_urls[k])\n",
    "print(\"================================\")\n",
    "print(\"선택한 모든 이미지 내려받기 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kfc download complete\n",
      "done!!\n"
     ]
    }
   ],
   "source": [
    "## 선언\n",
    "import os\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "baseUrl = 'https://search.naver.com/search.naver?where=image&sm=tab_jum&query='\n",
    "\n",
    "\n",
    "save_root = 'C:\\myPycode\\download'\n",
    "if not os.path.exists(save_root):os.makedirs(save_root)\n",
    "\n",
    "def get_images(query='apple', limit=20):\n",
    "    save_path = os.path.join(save_root, query)\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    # 한글 검색 자동 변환\n",
    "    url = baseUrl + quote_plus(query)\n",
    "    html = urlopen(url)\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    img = soup.find_all(class_='_img', limit=limit)\n",
    "\n",
    "    n = 1\n",
    "    for i in img:\n",
    "        imgUrl = i['data-source']\n",
    "        with urlopen(imgUrl) as f:\n",
    "            with open(os.path.join(save_path, str(n)+'.jpg'),'wb') as h: # w - write b - binary\n",
    "                img = f.read()\n",
    "                h.write(img)\n",
    "        n += 1\n",
    "    print('%s download complete' % (query))\n",
    "    \n",
    "queries =  ['kfc']\n",
    "\n",
    "num_limit = 1100\n",
    "    \n",
    "for query in queries:\n",
    "    get_images(query=query, limit=num_limit)\n",
    "    \n",
    "print('done!!');beep = lambda x: os.system(\"echo -n '\\a';sleep 0.3;\" * x);beep(3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://www.python.org/static/img/python-logo.png'\n",
    "html_image = requests.get(url)\n",
    "html_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python-logo.png'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "image_file_name = os.path.basename(url)\n",
    "image_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/myPyCode/download\\\\python-logo.png'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = os.path.join(folder, image_file_name)\n",
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageFile = open(image_path, 'wb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "chujnk_size = 1000000\n",
    "for chunk in html_image.iter_content(chunk_size):\n",
    "    imageFile.write(chunk)\n",
    "imageFile.close()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
